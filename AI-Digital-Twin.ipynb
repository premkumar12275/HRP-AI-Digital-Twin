{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43358e70",
   "metadata": {},
   "source": [
    "# This notebook is part of the AI Digital Twin project.\n",
    "# It is designed to be run in a Jupyter Notebook environment.\n",
    "# The code is structured to process ICU data, extract features, and train models.\n",
    "# download the MIMIC-III data files before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849a841",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a596bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required packages:\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c437af0",
   "metadata": {},
   "source": [
    "*** Setup and paths ***\n",
    "Download MIMIC-III csv data files. \n",
    "ICUSTAYS, PRESCRIPTIONS, MICROBIOLOGYEVENTS, CHARTEVENTS, LABEVENTS files are used. save these files to input folder and provide the path in below setup code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6ac061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICUSTAYS: OK → input/ICUSTAYS.csv.gz\n",
      "CHARTEVENTS: OK → input/CHARTEVENTS.csv.gz\n",
      "LABEVENTS: OK → input/LABEVENTS.csv.gz\n",
      "SEPSIS: OK → ./output/sepsis_labels.csv\n",
      "CONTROL: OK → ./output/control_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os, json, math, gc, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data / ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \".\"\n",
    "\n",
    "RAW = {\n",
    "    \"ICUSTAYS\": f\"input/ICUSTAYS.csv.gz\",\n",
    "    \"CHARTEVENTS\": f\"input/CHARTEVENTS.csv.gz\",   # or .csv\n",
    "    \"LABEVENTS\": f\"input/LABEVENTS.csv.gz\",\n",
    "}\n",
    "\n",
    "OUT = f\"{DATA_DIR}/output\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "LABELS = {\n",
    "    \"SEPSIS\": f\"{OUT}/sepsis_labels.csv\",\n",
    "    \"CONTROL\": f\"{OUT}/control_labels.csv\",\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "\n",
    "# Quick sanity\n",
    "for k,v in {**RAW, **LABELS}.items():\n",
    "    print(f\"{k}: {'OK' if os.path.exists(v) else 'MISSING'} → {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9d3d6",
   "metadata": {},
   "source": [
    "Filter CHARTEVENTS to vital ITEMIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1881b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18244797, 4) → saved to output/filtered_chartevents.csv.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ITEM_IDS = {\n",
    "    \"Heart Rate\": [211],\n",
    "    \"Systolic BP\": [51],\n",
    "    \"Diastolic BP\": [8368],\n",
    "    \"Mean BP\": [456],\n",
    "    \"Respiratory Rate\": [618],\n",
    "    \"SpO2\": [646],\n",
    "    \"Temperature\": [223761],\n",
    "}\n",
    "\n",
    "use_itemids = sorted({i for ids in ITEM_IDS.values() for i in ids})\n",
    "usecols = [\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"VALUENUM\"]\n",
    "\n",
    "# If you already have filtered_chartevents.csv.gz, skip this cell.\n",
    "ce = pd.read_csv(RAW[\"CHARTEVENTS\"], usecols=usecols, parse_dates=[\"CHARTTIME\"])\n",
    "ce = ce[ce[\"ITEMID\"].isin(use_itemids)]\n",
    "ce.to_csv(f\"{OUT}/filtered_chartevents.csv.gz\", index=False, compression=\"gzip\")\n",
    "print(ce.shape, \"[INFO] saved to output/filtered_chartevents.csv.gz\")\n",
    "del ce; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea6271",
   "metadata": {},
   "source": [
    "2) Vitals feature extraction (12h window) - sepsis & controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_vitals(labels_file, out_file, chart_file=f\"{OUT}/filtered_chartevents.csv.gz\"):\n",
    "    chart = pd.read_csv(chart_file, parse_dates=[\"CHARTTIME\"])\n",
    "    labels = pd.read_csv(labels_file)\n",
    "    labels[\"SEPSIS_ONSET\"] = pd.to_datetime(labels[\"SEPSIS_ONSET\"])\n",
    "\n",
    "    feats = []\n",
    "    for _, r in tqdm(labels.iterrows(), total=len(labels)):\n",
    "        icu, onset = r[\"ICUSTAY_ID\"], r[\"SEPSIS_ONSET\"]\n",
    "        start = onset - pd.Timedelta(hours=12)\n",
    "        win = chart[(chart[\"ICUSTAY_ID\"]==icu) &\n",
    "                    (chart[\"CHARTTIME\"]>=start) &\n",
    "                    (chart[\"CHARTTIME\"]<=onset)]\n",
    "\n",
    "        row = {\"ICUSTAY_ID\": icu}\n",
    "        for name, ids in ITEM_IDS.items():\n",
    "            vals = win[win[\"ITEMID\"].isin(ids)][\"VALUENUM\"].dropna()\n",
    "            row[f\"{name}_mean\"]  = vals.mean() if not vals.empty else np.nan\n",
    "            row[f\"{name}_std\"]   = vals.std()  if not vals.empty else np.nan\n",
    "            row[f\"{name}_delta\"] = (vals.iloc[-1]-vals.iloc[0]) if len(vals)>1 else np.nan\n",
    "        feats.append(row)\n",
    "\n",
    "    df = pd.DataFrame(feats)\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"[INFO] Saved {out_file} | shape={df.shape}\")\n",
    "\n",
    "extract_vitals(LABELS[\"SEPSIS\"],  f\"{OUT}/features_robust.csv\")\n",
    "extract_vitals(LABELS[\"CONTROL\"], f\"{OUT}/control_features_robust.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871306cb",
   "metadata": {},
   "source": [
    "3) Lab feature extraction (12h window; join Labevents and icustays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18808cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB_ITEMS = {  # MIMIC-III codes\n",
    "    \"Lactate\": [50813],\n",
    "    \"WBC\": [51300],\n",
    "    \"Creatinine\": [50912],\n",
    "    \"Bilirubin\": [50885],\n",
    "    \"Platelets\": [51265],\n",
    "}\n",
    "\n",
    "def extract_labs(labels_file, out_file):\n",
    "    labs = pd.read_csv(RAW[\"LABEVENTS\"], usecols=[\"HADM_ID\",\"ITEMID\",\"CHARTTIME\",\"VALUENUM\"],\n",
    "                       parse_dates=[\"CHARTTIME\"])\n",
    "    icu = pd.read_csv(RAW[\"ICUSTAYS\"], usecols=[\"ICUSTAY_ID\",\"HADM_ID\",\"INTIME\",\"OUTTIME\"],\n",
    "                      parse_dates=[\"INTIME\",\"OUTTIME\"])\n",
    "    labs = labs.merge(icu, on=\"HADM_ID\", how=\"inner\")\n",
    "    # keep only labs within the ICU stay\n",
    "    labs = labs[(labs[\"CHARTTIME\"]>=labs[\"INTIME\"]) & (labs[\"CHARTTIME\"]<=labs[\"OUTTIME\"])]\n",
    "    labels = pd.read_csv(labels_file); labels[\"SEPSIS_ONSET\"] = pd.to_datetime(labels[\"SEPSIS_ONSET\"])\n",
    "\n",
    "    rows=[]\n",
    "    for _, r in tqdm(labels.iterrows(), total=len(labels)):\n",
    "        icustay, onset = r[\"ICUSTAY_ID\"], r[\"SEPSIS_ONSET\"]\n",
    "        start = onset - pd.Timedelta(hours=12)\n",
    "        win = labs[(labs[\"ICUSTAY_ID\"]==icustay) &\n",
    "                   (labs[\"CHARTTIME\"]>=start) &\n",
    "                   (labs[\"CHARTTIME\"]<=onset)]\n",
    "        row={\"ICUSTAY_ID\": icustay}\n",
    "        for name, ids in LAB_ITEMS.items():\n",
    "            v = win[win[\"ITEMID\"].isin(ids)][\"VALUENUM\"].dropna()\n",
    "            row[f\"{name}_mean\"]=v.mean() if not v.empty else np.nan\n",
    "            row[f\"{name}_std\"] =v.std()  if not v.empty else np.nan\n",
    "            row[f\"{name}_delta\"]=(v.iloc[-1]-v.iloc[0]) if len(v)>1 else np.nan\n",
    "        rows.append(row)\n",
    "\n",
    "    df=pd.DataFrame(rows)\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"[INFO] Saved {out_file} | shape={df.shape}\")\n",
    "\n",
    "extract_labs(LABELS[\"SEPSIS\"],  f\"{OUT}/lab_features_sepsis.csv\")\n",
    "extract_labs(LABELS[\"CONTROL\"], f\"{OUT}/lab_features_controls.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8db7a",
   "metadata": {},
   "source": [
    "4) Merge vitals + labs + labels -> model ready csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28632377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sev = pd.read_csv(f\"{OUT}/features_robust.csv\").merge(\n",
    "    pd.read_csv(f\"{OUT}/lab_features_sepsis.csv\"), on=\"ICUSTAY_ID\", how=\"left\"\n",
    ").assign(label=1)\n",
    "\n",
    "ctl = pd.read_csv(f\"{OUT}/control_features_robust.csv\").merge(\n",
    "    pd.read_csv(f\"{OUT}/lab_features_controls.csv\"), on=\"ICUSTAY_ID\", how=\"left\"\n",
    ").assign(label=0)\n",
    "\n",
    "data = pd.concat([sev, ctl], ignore_index=True)\n",
    "print(\"[INFO] Before impute:\", data.shape, \"missing:\", data.isna().sum().sum())\n",
    "data = data.fillna(data.mean(numeric_only=True))\n",
    "data = data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "data.to_csv(f\"{OUT}/merged_dataset_with_labs.csv\", index=False)\n",
    "data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7ee61",
   "metadata": {},
   "source": [
    "5) Train & evaluate XGBoost (save model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6225b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import joblib, os\n",
    "\n",
    "df = pd.read_csv(f\"{OUT}/merged_dataset_with_labs.csv\")\n",
    "X = df.drop(columns=[\"ICUSTAY_ID\",\"label\"], errors=\"ignore\")\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, stratify=y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "clf = XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05, subsample=0.9,\n",
    "                    colsample_bytree=0.9, random_state=SEED, n_jobs=-1)\n",
    "clf.fit(X_tr, y_tr)\n",
    "p = clf.predict_proba(X_te)[:,1]\n",
    "\n",
    "print(\"[INFO] AUROC :\", round(roc_auc_score(y_te, p), 4))\n",
    "print(\"[INFO] AUPRC :\", round(average_precision_score(y_te, p), 4))\n",
    "print(classification_report(y_te, p>0.5))\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(clf, \"models/xgb_with_labs.joblib\"); print(\"Saved models/xgb_with_labs.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542c261",
   "metadata": {},
   "source": [
    "6) SHAP (summary plot +3 force plats as html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap, matplotlib.pyplot as plt\n",
    "explainer = shap.Explainer(clf)\n",
    "sv = explainer(X)  # on full feature set for global importance\n",
    "\n",
    "shap.summary_plot(sv, X, show=False)\n",
    "plt.tight_layout(); plt.savefig(f\"{OUT}/shap_summary_with_labs.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "shap.initjs()\n",
    "for i in range(3):\n",
    "    fp = shap.force_plot(explainer.expected_value, sv[i].values, X.iloc[i], feature_names=X.columns)\n",
    "    with open(f\"{OUT}/force_plot_with_labs_{i}.html\", \"w\") as f: f.write(fp.html())\n",
    "print(\"[INFO] Saved SHAP outputs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb508b8d",
   "metadata": {},
   "source": [
    "7) Build time-series tensors (vitals-only) for GRU-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild from filtered_chartevents for both labels\n",
    "chart = pd.read_csv(f\"{OUT}/filtered_chartevents.csv.gz\", parse_dates=[\"CHARTTIME\"])\n",
    "lab_all = []\n",
    "lab_all.append(pd.read_csv(LABELS[\"SEPSIS\"]).assign(label=1))\n",
    "lab_all.append(pd.read_csv(LABELS[\"CONTROL\"]).assign(label=0))\n",
    "labels_all = pd.concat(lab_all, ignore_index=True)\n",
    "labels_all[\"SEPSIS_ONSET\"] = pd.to_datetime(labels_all[\"SEPSIS_ONSET\"])\n",
    "\n",
    "FEATURES = list(ITEM_IDS.keys())\n",
    "WINDOW_HOURS = 12\n",
    "D = len(FEATURES)\n",
    "\n",
    "X_list, M_list, Y_list = [], [], []\n",
    "\n",
    "for _, r in tqdm(labels_all.iterrows(), total=len(labels_all)):\n",
    "    icu, onset = r[\"ICUSTAY_ID\"], r[\"SEPSIS_ONSET\"]\n",
    "    start = onset - pd.Timedelta(hours=WINDOW_HOURS)\n",
    "    df = chart[(chart[\"ICUSTAY_ID\"]==icu) & (chart[\"CHARTTIME\"]>=start) & (chart[\"CHARTTIME\"]<=onset)].copy()\n",
    "    df[\"hour\"] = ((df[\"CHARTTIME\"]-start).dt.total_seconds()//3600).astype(int)\n",
    "\n",
    "    Xp = np.full((WINDOW_HOURS,D), np.nan, dtype=float)\n",
    "    Mp = np.zeros_like(Xp)\n",
    "\n",
    "    for j,f in enumerate(FEATURES):\n",
    "        ids = ITEM_IDS[f]\n",
    "        for h in range(WINDOW_HOURS):\n",
    "            vals = df[(df[\"ITEMID\"].isin(ids)) & (df[\"hour\"]==h)][\"VALUENUM\"].dropna()\n",
    "            if not vals.empty:\n",
    "                Xp[h,j] = vals.mean(); Mp[h,j] = 1\n",
    "\n",
    "    X_list.append(Xp); M_list.append(Mp); Y_list.append(r[\"label\"])\n",
    "\n",
    "X_ts = np.array(X_list); M_ts = np.array(M_list); y_ts = np.array(Y_list)\n",
    "np.save(f\"{OUT}/timeseries_X.npy\", X_ts)\n",
    "np.save(f\"{OUT}/timeseries_M.npy\", M_ts)\n",
    "np.save(f\"{OUT}/timeseries_labels.npy\", y_ts)\n",
    "X_ts.shape, M_ts.shape, y_ts.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4fef0",
   "metadata": {},
   "source": [
    "8) Train GRU-D (CPU friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Minimal GRU-D style block (mean-impute + learned gates)\n",
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma_x = nn.Parameter(torch.ones(input_size))\n",
    "        self.z = nn.Linear(input_size*3, hidden_size)\n",
    "        self.r = nn.Linear(input_size*3, hidden_size)\n",
    "        self.h_tilde = nn.Linear(input_size*3, hidden_size)\n",
    "        self.h_out = nn.Linear(hidden_size, 1)\n",
    "        self.input_means = None\n",
    "\n",
    "    def forward(self, X, M):\n",
    "        B,T,D = X.shape\n",
    "        if self.input_means is None:\n",
    "            raise ValueError(\"Set model.input_means tensor first\")\n",
    "        device = X.device\n",
    "        h = torch.zeros(B, self.hidden_size, device=device)\n",
    "        Xmean = self.input_means.unsqueeze(0).unsqueeze(0).expand(B,T,D)\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t, m_t = X[:,t], M[:,t]\n",
    "            x_hat = m_t*x_t + (1-m_t)*Xmean[:,t]\n",
    "            x_gamma = torch.exp(-self.gamma_x*(1-m_t))\n",
    "            x_tilde = x_gamma*x_hat + (1-x_gamma)*Xmean[:,t]\n",
    "            inp = torch.cat([x_tilde, Xmean[:,t], m_t], dim=1)\n",
    "            zt = torch.sigmoid(self.z(inp)); rt = torch.sigmoid(self.r(inp))\n",
    "            h_t = torch.tanh(self.h_tilde(inp))\n",
    "            h = (1-zt)*h + zt*h_t\n",
    "        out = torch.sigmoid(self.h_out(h)).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Load tensors\n",
    "X = np.load(f\"{OUT}/timeseries_X.npy\")\n",
    "M = np.load(f\"{OUT}/timeseries_M.npy\")\n",
    "y = np.load(f\"{OUT}/timeseries_labels.npy\")\n",
    "\n",
    "# Torch tensors\n",
    "X = torch.tensor(np.nan_to_num(X, nan=0.0), dtype=torch.float32)\n",
    "M = torch.tensor(M, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_tr, X_va, M_tr, M_va, y_tr, y_va = train_test_split(X, M, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "model = GRUD(input_size=X.shape[2], hidden_size=64)\n",
    "train_mean = torch.tensor(np.nanmean(X_tr.numpy(), axis=(0,1)), dtype=torch.float32)\n",
    "train_mean = torch.nan_to_num(train_mean, nan=0.0)\n",
    "model.input_means = train_mean\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "EPOCHS = 20; BATCH=64\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); idx = torch.randperm(X_tr.size(0)); epoch_loss=0.0\n",
    "    for i in range(0, X_tr.size(0), BATCH):\n",
    "        b = idx[i:i+BATCH]\n",
    "        opt.zero_grad()\n",
    "        out = model(X_tr[b], M_tr[b])\n",
    "        loss = loss_fn(out, y_tr[b])\n",
    "        loss.backward(); opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "    # val\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p = model(X_va, M_va).numpy()\n",
    "        auroc = roc_auc_score(y_va.numpy(), p)\n",
    "        auprc = average_precision_score(y_va.numpy(), p)\n",
    "    print(f\"Epoch {ep:02d} | Loss {epoch_loss:.3f} | AUROC {auroc:.3f} | AUPRC {auprc:.3f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"models/grud_model.pt\"); print(\"Saved models/grud_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8847dc",
   "metadata": {},
   "source": [
    "9) GRU-D evaluation plots (ROC/PR) + risk trajetories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852650cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(X_va, M_va).numpy(); yy = y_va.numpy()\n",
    "\n",
    "auroc = roc_auc_score(yy, p)\n",
    "auprc = average_precision_score(yy, p)\n",
    "fpr,\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hc_a6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
